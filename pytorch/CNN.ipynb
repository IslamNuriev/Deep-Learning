{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7773f4",
   "metadata": {},
   "source": [
    "# Сверточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5e0ca",
   "metadata": {},
   "source": [
    "Когда речь заходит об обработке изображений, то используется особая архитектура нейронных сетей - Сверточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1bf81",
   "metadata": {},
   "source": [
    "Каждый фильтр сканирует все пространство входного сигнала. CNN инвариантны к местоположению объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d474b",
   "metadata": {},
   "source": [
    "Вычисленные признаки (n_channels) предполагается анализировать на более крупном масштабе:\n",
    "Для этого размерность карт признаков сокращают с помощью одной из операций:\n",
    "\n",
    "- MaxPooling - отбор наибольших значений;\n",
    "- MinPooling - отбор наименьших значений;\n",
    "- AveragePooling - отбор средних значений;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c0aa0",
   "metadata": {},
   "source": [
    "После MaxPooling карта признаков сокращается в 2 раза. Зачем это нужно? Для обобщения, цель выполнять анализ признаков на все более крупном масштабе. Получаем анализ данных на более крупном масштабе и благодаря этому нейроны следующего слоя способны выделять более общие признаки на изображении."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd904de",
   "metadata": {},
   "source": [
    "Идея: Двумерный входной сигнал изображения пропускается через фильтр. Фильтр имеет веса + bias. Далее накладываем фильтр на входной сигнал и формируем выходное значени:\n",
    "\n",
    "$$z_{11} = \\omega_{11} \\cdot x_{11} + \\omega_{12} \\cdot x_{12} + ... + \\omega_{33} \\cdot x_{33} + \\omega_0$$\n",
    "\n",
    "$$u_{11} = \\sigma(z_{11})$$\n",
    "\n",
    " - получаем 1-е выходное значение $u_{11}$ и далее смешаем окно и считаем следующее число. В результате получаем **карту признаков**. Процесс продолжается с другим ядром (фильтром). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba3ee5",
   "metadata": {},
   "source": [
    "Классы сверточнных слоев:\n",
    "- nn.Conv1d - для одномерной свертки\n",
    "- nn.Conv2d - для двумерной свертки (например, изображение)\n",
    "- nn.Conv3d - для трехмерной свертки (например, снимок МРТ)\n",
    "\n",
    "-----------------------------------------------------------\n",
    "\n",
    "- nn.MaxPool1d - для одномерного сигнала\n",
    "- nn.MaxPool2d - для двумерного сигнала\n",
    "- nn.MaxPool3d - для трехмерного сигнала"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53c18c",
   "metadata": {},
   "source": [
    "## Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609ded5",
   "metadata": {},
   "source": [
    "На вход сверточного слоя, который определяется классом Conv2d, должен подаваться тензор в следующем формате:\n",
    "(batch, channels, H, W)\n",
    "\n",
    "- batch - размер пакета\n",
    "- channels - число каналов в входном тензоре\n",
    "- H - число строк\n",
    "- W - число столбцов\n",
    "\n",
    "\n",
    "Сам класс Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
    "\n",
    "- если подается полноценное изображение в цветах RGB, nj in_channels = 3, если изображение в градациях серого, то in_channels = 1\n",
    "- out_channels - число выходных каналов (число карты признаков), при этом каждая карта, которая является результатом применения фильтра к входному тензору в общем случае будет иметь размерность $H_m, W_m$, которые меньше, чем входные размеры\n",
    "\n",
    "На выходе будем иметь размерность: (batch, out_channels, $H_m, W_m$)\n",
    "- kernel_size - размер сверточного слоя, то есть все ядра в количестве out_channels будут иметь один размер;\n",
    "\n",
    "А результирующий размер каждого ядра: (in_channels, kernel_size)\n",
    "\n",
    "- stride - шаг сканирования фильтра (default: stride = 1)\n",
    "- padding - расширение ядра (добавление нулей к входному тензору, default: padding=0)\n",
    "\n",
    "Если padding = 'same', то размеры карт-признаков будут такими же, как и размеры входного тензора, то есть $H_m = H, W_m = W$\n",
    "\n",
    "- bias к сумме добавляется смещение:\n",
    "\n",
    "$$z_{n,m} = \\omega_0 + \\sum_{k=1}^{ch} \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\omega_{k,i,j} \\cdot x_{k,i+n-1,j+m-2}$$\n",
    "\n",
    "В ряде случаев, bias=False стоит убирать, когда используется batch_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e3085",
   "metadata": {},
   "source": [
    "\n",
    "MaxPool2d(kernel_size, stried=None) часто применяют после сверточных слоев:\n",
    "\n",
    "- kernel_size - размер области выделения\n",
    "- stried - задает смещение областей\n",
    "\n",
    "на выходе будем иметь размерность: (batch, channels, H // 2, W // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92a0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms.v2 as tfs\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SunDataset(data.Dataset):\n",
    "    def __init__(self, path, train=True, transform=None):\n",
    "        self.path = os.path.join(path, \"train\" if train else \"test\")\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(os.path.join(self.path, \"format.json\"), \"r\") as fp:\n",
    "            self.format = json.load(fp)\n",
    "\n",
    "        self.length = len(self.format)\n",
    "        self.files = tuple(self.format.keys())\n",
    "        self.targets = tuple(self.format.values())\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        path_file = os.path.join(self.path, self.files[item])\n",
    "        img = Image.open(path_file).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(self.targets[item], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6192cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tfs.Compose([tfs.ToImage(),\n",
    "                          tfs.ToDtype(torch.float32, scale=True)])\n",
    "d_train = SunDataset(\"dataset_reg\", transform=transforms)\n",
    "train_data = data.DataLoader(d_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db329c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,32,3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2), # (batch, 32, 128, 128)\n",
    "    nn.Conv2d(32,8,3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2), # (batch, 8, 64, 64)\n",
    "    nn.Conv2d(8,4,3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),  # (batch, 4, 32, 32)\n",
    "    nn.Flatten(), # этот слой способен вытянуть все эти элементы (4, 32, 32) в единый вектор\n",
    "    nn.Linear(4096, 128), # (batch, 4096 )\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137aaf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e72243b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, loss_mean=3135.721: 100%|██████████| 313/313 [03:47<00:00,  1.37it/s]\n",
      "Epoch 2/5, loss_mean=42.658: 100%|██████████| 313/313 [03:43<00:00,  1.40it/s]\n",
      "Epoch 3/5, loss_mean=18.702: 100%|██████████| 313/313 [03:57<00:00,  1.32it/s]\n",
      "Epoch 4/5, loss_mean=13.784: 100%|██████████| 313/313 [03:39<00:00,  1.42it/s]\n",
      "Epoch 5/5, loss_mean=9.595: 100%|██████████| 313/313 [03:32<00:00,  1.47it/s] \n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "model.train()\n",
    "\n",
    "for _e in range(epochs):\n",
    "    loss_mean = 0\n",
    "    lm_count = 0\n",
    "\n",
    "    train_tqdm = tqdm(train_data, leave=True)\n",
    "    for x_train, y_train in train_tqdm:\n",
    "        predict = model(x_train)\n",
    "        loss = loss_function(predict, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lm_count += 1\n",
    "        loss_mean = 1/lm_count * loss.item() + (1 - 1/lm_count) * loss_mean\n",
    "\n",
    "        train_tqdm.set_description(f\"Epoch {_e+1}/{epochs}, loss_mean={loss_mean:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c8917d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = model.state_dict()\n",
    "torch.save(st,'model_sun.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edbfe73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = SunDataset(\"dataset_reg\", train=False, transform=transforms)\n",
    "test_data = data.DataLoader(d_test, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56d67125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:10<?, ?it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.398188281059265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# тестирование НС\n",
    "Q = 0\n",
    "count = 0\n",
    "model.eval()\n",
    "\n",
    "test_tqdm = tqdm(test_data, leave=True)\n",
    "for x_test, y_test in test_tqdm:\n",
    "    with torch.no_grad():\n",
    "        p = model(x_test)\n",
    "        Q += loss_function(p, y_test).item()\n",
    "        count += 1\n",
    "\n",
    "Q /= count\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85176dd6",
   "metadata": {},
   "source": [
    "## VGG-16, VGG-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b769c4",
   "metadata": {},
   "source": [
    "Visual Geometry Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f9d91",
   "metadata": {},
   "source": [
    "from t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmm-tools-9U9JP_nD-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
